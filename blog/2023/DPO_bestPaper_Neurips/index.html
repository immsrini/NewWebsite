<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DPO v. PPO | Mukundhan Srinivasan </title> <meta name="author" content="Mukundhan Srinivasan"> <meta name="description" content="Clearly only one winner here"> <meta name="keywords" content="Mukund, Mukundhan, Srinivasan, immsrini"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://immsrini.github.io/blog/2023/DPO_bestPaper_Neurips/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Mukundhan Srinivasan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">DPO v. PPO</h1> <p class="post-meta"> December 28, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/machinelearning"> <i class="fa-solid fa-hashtag fa-sm"></i> MachineLearning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Given the complexity and specificity of Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO) in the context of Large Language Models (LLMs), it’s important to delve into recent research and developments to provide an accurate and detailed report. Here’s an overview:</p> <h4 id="direct-preference-optimization-dpo-in-llms">Direct Preference Optimization (DPO) in LLMs</h4> <p>Overview:</p> <p>Direct Preference Optimization is a method used to align the outputs of models like LLMs with human preferences or specified criteria. This method involves collecting human feedback on model outputs and directly optimizing the model to produce outputs that are more aligned with these preferences.</p> <p>Key Features:</p> <p>Human-in-the-loop: DPO relies heavily on human feedback, making it particularly suitable for tasks where human judgment is crucial. Alignment with Preferences: This method is effective in fine-tuning models to adhere to nuanced human preferences, ethics, or cultural norms.</p> <p>Examples and Research:</p> <p>A significant example of this approach is in fine-tuning language models for tasks like generating more ethical or unbiased content. Research papers like “Learning to Summarize with Human Feedback” (OpenAI) illustrate the use of human feedback in optimizing language models.</p> <h4 id="proximal-policy-optimization-ppo-in-llms">Proximal Policy Optimization (PPO) in LLMs</h4> <p>Overview:</p> <p>PPO is a reinforcement learning algorithm that’s been adapted for fine-tuning LLMs, particularly in decision-making or interactive scenarios. The algorithm is known for its stability and effectiveness in a variety of environments, making it a strong choice for fine-tuning language models.</p> <p>Key Features:</p> <p>Stable and Efficient Learning: PPO is designed to avoid large policy updates, which can destabilize training. Broad Applicability: The algorithm can be applied to a variety of tasks, from game playing to conversational agents.</p> <p>Examples and Research:</p> <p>An application of PPO in LLMs could be in interactive systems like chatbots, where the model learns to optimize its responses based on rewards. “Proximal Policy Optimization Algorithms” (Schulman et al., 2017) provides foundational knowledge on PPO’s mechanics and applications.</p> <h4 id="comparison-and-analysis">Comparison and Analysis</h4> <ol> <li> <p>Suitability for LLMs: DPO is particularly suited for tasks where alignment with complex human preferences and judgments is crucial. PPO, on the other hand, excels in environments where a clear reward signal can guide the optimization of responses or actions.</p> </li> <li> <p>Challenges and Limitations: DPO’s reliance on human feedback makes it resource-intensive and potentially biased based on the feedback sample. PPO, while efficient, might not capture the nuanced preferences that human feedback can provide, as seen in DPO.</p> </li> <li> <p>Choosing Between DPO and PPO: The choice between DPO and PPO depends on the specific requirements of the task at hand. For applications requiring adherence to complex human values or preferences, DPO might be preferable. For scenarios where there are clear reward structures and the need for efficient learning, PPO could be more suitable.</p> </li> </ol> <h4 id="conclusion">Conclusion</h4> <p>Both DPO and PPO offer unique advantages in the context of fine-tuning LLMs, with their suitability depending on the specific goals and constraints of the application. DPO excels in aligning model outputs with nuanced human preferences, while PPO offers a more general and efficient approach to optimizing model behavior in interactive scenarios.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/YearOfRAGs/">Leveraging Retrieval-Augmented Generation (RAG) with Large Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/The-Intersection-of-AI,-Society,-and-the-Economy/">How AI and Large Language Models Forge the Future of Value</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/funcProg/">Embracing Functional Programming</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/BeautyofCalc/">The Elegance of Calculus</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/HegalsPOS/">Navigating the Depths of Hegel's Phenomenology of Spirit</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Mukundhan Srinivasan. Powered by <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> and hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: March 24, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-42D6SVWV0W"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-42D6SVWV0W");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>